# fine_tuning/config.yaml

# --- 模型和数据集配置 ---
model_config:
  # Hugging Face上的模型名称或本地路径
  name: "/home/user02/DeepSeek-R1-0528-Qwen3-8B" # 从魔搭社区上下载
  torch_dtype: "float16"
  device_map: "auto"

dataset_config:
  # 构建结构一样的json文件
  # 数据集格式 ('alpaca' or 'chat')
  format: "alpaca"
  # 数据集路径
  train_path: "/home/user02/clean_data"
  validation_path: ""
  # Alpaca格式中的响应模板
  alpaca_response_template: "### Response:"
  # ChatML/Qwen格式中的响应模板 (注意：tokenizer会自动添加换行符，所以这里不需要)
  chat_response_template: "<|im_start|>assistant"
  # 将数据集中多少比例划分为测试集（如果不存在）
  test_size: 0.1
  # Tokenizer处理的最大长度
  max_length: 4096

# --- 新增: 量化配置 (BitsAndBytes) ---
quantization_config:
  # 设置为 true 启用 QAT (量化感知训练)
  enabled: false
  # 4-bit量化
  load_in_4bit: true
  # bnb量化类型 (fp4 or nf4)
  bnb_4bit_quant_type: "nf4"
  # 计算时使用的数据类型 (bfloat16 or float16)
  bnb_4bit_compute_dtype: "bfloat16"
  # 是否使用双重量化
  bnb_4bit_use_double_quant: true

# --- PEFT (参数高效微调) 配置 ---
peft_config:
  # 设置为 true 启用 PEFT (如 LoRA), 设置为 false 或移除 peft_config 则启用全参数微调
  enabled: true
  # 指定要使用的PEFT方法，必须是 PEFT_CONFIG_MAPPING 中支持的方法 (当前为 "lora")
  method: "lora"
  # 对应方法的参数
  params:
    r: 16
    lora_alpha: 32
    lora_dropout: 0.05
    bias: "none"
    task_type: "CAUSAL_LM"
    target_modules:
      - "q_proj"
      - "v_proj"
      - "k_proj"
      - "o_proj"

# --- 训练参数 (Hugging Face TrainingArguments) ---
training_args:
  output_dir: "./qwen_lora_finetuned"
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  learning_rate: 2.0e-5
  weight_decay: 0.01
  num_train_epochs: 3
  logging_steps: 10
  save_strategy: "epoch"
  eval_strategy: "no"
  fp16: true
  gradient_checkpointing: true
  optim: "adamw_torch"
  warmup_ratio: 0.1

# --- 可视化和日志记录 ---
visualization:
  # Weights & Biases 项目名称
  wandb_project: "qwen-finetune-project"